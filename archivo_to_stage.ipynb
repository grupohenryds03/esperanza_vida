{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moludo para la ingesta de csv a la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### se crea el conector a la base de datos snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(\n",
    "    user='grupods03',\n",
    "    password='Henry2022#',\n",
    "    account='nr28668.sa-east-1.aws')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### se crea funcion para ejecutar querys a la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(connection, query):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"Use database LAKE\" # se inicializa la base de datos\n",
    "execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"use warehouse dw_ev\" # se inicializa el warehose con la tablas relacionales y el stage\n",
    "execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### se ingestan los csv con una compresi√≥n desde la web de github al stage de snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    df=pd.read_csv('https://raw.githubusercontent.com/grupohenryds03/esperanza_vida/main/datasets/contienente_limpio.csv')\n",
    "    df.drop('Unnamed: 0',inplace=True, axis=1)\n",
    "    df.to_csv(temp_dir +'/continente_limpio.csv', index=False)\n",
    "    sql = f\"PUT file://{temp_dir+'/continente4.csv'} @DATA_STAGE auto_compress=true\"\n",
    "    execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    df=pd.read_csv('https://raw.githubusercontent.com/grupohenryds03/esperanza_vida/main/datasets/hechos_limpio.csv')\n",
    "    df.drop('Unnamed: 0',inplace=True, axis=1)\n",
    "    df.to_csv(temp_dir +'/hechos_limpio.csv', index=False)\n",
    "    sql = f\"PUT file://{temp_dir+'/hechos_limpio.csv'} @DATA_STAGE auto_compress=true\"\n",
    "    execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    df=pd.read_csv('https://raw.githubusercontent.com/grupohenryds03/esperanza_vida/main/datasets/pais_limpio.csv')\n",
    "    df.drop('Unnamed: 0',inplace=True, axis=1)\n",
    "    df.to_csv(temp_dir +'/pais_limpio.csv', index=False)\n",
    "    sql = f\"PUT file://{temp_dir+'/pais_limpio.csv'} @DATA_STAGE auto_compress=true\"\n",
    "    execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    df=pd.read_csv('https://raw.githubusercontent.com/grupohenryds03/esperanza_vida/main/datasets/income_limpio.csv')\n",
    "    df.drop('Unnamed: 0',inplace=True, axis=1)\n",
    "    df.to_csv(temp_dir +'/income_limpio.csv', index=False)\n",
    "    sql = f\"PUT file://{temp_dir+'/income_limpio.csv'} @DATA_STAGE auto_compress=true\"\n",
    "    execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    df=pd.read_csv('https://raw.githubusercontent.com/grupohenryds03/esperanza_vida/main/datasets/indicador_limpio.csv')\n",
    "    df.drop('Unnamed: 0',inplace=True, axis=1)\n",
    "    df.to_csv(temp_dir +'/indicador_limpio.csv', index=False)\n",
    "    sql = f\"PUT file://{temp_dir+'/indicador_limpio.csv'} @DATA_STAGE auto_compress=true\"\n",
    "    execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lista de los csv comprimidos cargados al datalake de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql='list @DATA_STAGE'\n",
    "cur=conn.cursor()\n",
    "cur.execute(sql)\n",
    "for c in cur:\n",
    "    print(c)\n",
    "cur.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
