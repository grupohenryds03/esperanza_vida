{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se importa el conector de snowflake\n",
    "import snowflake.connector \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se crea conexión\n",
    "conn = snowflake.connector.connect(\n",
    "    user='grupods03',\n",
    "    password='Henry2022#',\n",
    "    account='nr28668.sa-east-1.aws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(connection, query):\n",
    "    cursor = connection.cursor() # se inicializa la conexión Creates a cursor object. Each statement will be executed in a new cursor object.\n",
    "    cursor.execute(query)\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"use database PRUEBA\" # se inicializa en database prueba\n",
    "\n",
    "execute_query(conn, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'create stage data_stage file_format = (type = \"csv\" field_delimiter = \",\" skip_header = 1)'\n",
    "execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '/Users/josetoledo/Desktop/prueba/df_hecho.csv'\n",
    "sql = \"PUT file://\" + csv_file + \" @DATA_STAGE auto_compress=true\"\n",
    "    \n",
    "execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tempfile\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (5) does not match length of index (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame([(\u001b[39m1\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mAmerica\u001b[39;49m\u001b[39m'\u001b[39;49m), (\u001b[39m2\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mEuropa\u001b[39;49m\u001b[39m'\u001b[39;49m), (\u001b[39m3\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mAsia\u001b[39;49m\u001b[39m'\u001b[39;49m),(\u001b[39m4\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mAfrica\u001b[39;49m\u001b[39m'\u001b[39;49m),(\u001b[39m5\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mOceania\u001b[39;49m\u001b[39m'\u001b[39;49m)], columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mID_CONTINENTE\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mCONTINENTE\u001b[39;49m\u001b[39m'\u001b[39;49m], index\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mID_CONTINENTE\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/Desktop/airflow_venv/lib/python3.10/site-packages/pandas/core/frame.py:729\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    720\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    722\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    727\u001b[0m         dtype,\n\u001b[1;32m    728\u001b[0m     )\n\u001b[0;32m--> 729\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    730\u001b[0m         arrays,\n\u001b[1;32m    731\u001b[0m         columns,\n\u001b[1;32m    732\u001b[0m         index,\n\u001b[1;32m    733\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    734\u001b[0m         typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[1;32m    735\u001b[0m     )\n\u001b[1;32m    736\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    737\u001b[0m     mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[1;32m    738\u001b[0m         data,\n\u001b[1;32m    739\u001b[0m         index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    743\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    744\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/airflow_venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:125\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    122\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n\u001b[1;32m    124\u001b[0m     \u001b[39m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     arrays \u001b[39m=\u001b[39m _homogenize(arrays, index, dtype)\n\u001b[1;32m    126\u001b[0m     \u001b[39m# _homogenize ensures\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[39m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[39m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/Desktop/airflow_venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:628\u001b[0m, in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    623\u001b[0m             val \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mfast_multiget(val, oindex\u001b[39m.\u001b[39m_values, default\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mnan)\n\u001b[1;32m    625\u001b[0m         val \u001b[39m=\u001b[39m sanitize_array(\n\u001b[1;32m    626\u001b[0m             val, index, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, raise_cast_failure\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    627\u001b[0m         )\n\u001b[0;32m--> 628\u001b[0m         com\u001b[39m.\u001b[39;49mrequire_length_match(val, index)\n\u001b[1;32m    630\u001b[0m     homogenized\u001b[39m.\u001b[39mappend(val)\n\u001b[1;32m    632\u001b[0m \u001b[39mreturn\u001b[39;00m homogenized\n",
      "File \u001b[0;32m~/Desktop/airflow_venv/lib/python3.10/site-packages/pandas/core/common.py:557\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    558\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    562\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (5) does not match length of index (1)"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([(1,'America'), (2,'Europa'), (3,'Asia'),(4,'Africa'),(5,'Oceania')], columns=['ID_CONTINENTE', 'CONTINENTE']) # se crea dataframe con pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_CONTINENTE</th>\n",
       "      <th>CONTINENTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Europa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Oceania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_CONTINENTE CONTINENTE\n",
       "0              1    America\n",
       "1              2     Europa\n",
       "2              3       Asia\n",
       "3              4     Africa\n",
       "4              5    Oceania"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=df.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/temp/tmp8lhsuaey'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mwith\u001b[39;00m tempfile\u001b[39m.\u001b[39;49mNamedTemporaryFile(delete\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39mas\u001b[39;00m temp:\n\u001b[1;32m      2\u001b[0m     df\u001b[39m.\u001b[39mto_csv(temp\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m     temp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/tempfile.py:559\u001b[0m, in \u001b[0;36mNamedTemporaryFile\u001b[0;34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[39mreturn\u001b[39;00m fd\n\u001b[1;32m    558\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 559\u001b[0m     file \u001b[39m=\u001b[39m _io\u001b[39m.\u001b[39;49mopen(\u001b[39mdir\u001b[39;49m, mode, buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[1;32m    560\u001b[0m                     newline\u001b[39m=\u001b[39;49mnewline, encoding\u001b[39m=\u001b[39;49mencoding, errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    561\u001b[0m                     opener\u001b[39m=\u001b[39;49mopener)\n\u001b[1;32m    562\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m         raw \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(file, \u001b[39m'\u001b[39m\u001b[39mbuffer\u001b[39m\u001b[39m'\u001b[39m, file)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/tempfile.py:556\u001b[0m, in \u001b[0;36mNamedTemporaryFile.<locals>.opener\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopener\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[1;32m    555\u001b[0m     \u001b[39mnonlocal\u001b[39;00m name\n\u001b[0;32m--> 556\u001b[0m     fd, name \u001b[39m=\u001b[39m _mkstemp_inner(\u001b[39mdir\u001b[39;49m, prefix, suffix, flags, output_type)\n\u001b[1;32m    557\u001b[0m     \u001b[39mreturn\u001b[39;00m fd\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/tempfile.py:256\u001b[0m, in \u001b[0;36m_mkstemp_inner\u001b[0;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[1;32m    254\u001b[0m _sys\u001b[39m.\u001b[39maudit(\u001b[39m\"\u001b[39m\u001b[39mtempfile.mkstemp\u001b[39m\u001b[39m\"\u001b[39m, file)\n\u001b[1;32m    255\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m     fd \u001b[39m=\u001b[39m _os\u001b[39m.\u001b[39;49mopen(file, flags, \u001b[39m0o600\u001b[39;49m)\n\u001b[1;32m    257\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[39mcontinue\u001b[39;00m    \u001b[39m# try again\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/temp/tmp8lhsuaey'"
     ]
    }
   ],
   "source": [
    "with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
    "    df.to_csv(temp.name + '.csv')\n",
    "    temp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "001003 (42000): SQL compilation error:\nsyntax error line 1 at position 4 unexpected 'ID_CONTINENTE'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [100], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sql \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPUT \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m @DATA_STAGE auto_compress=true\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m execute_query(conn, sql)\n",
      "Cell \u001b[0;32mIn [12], line 3\u001b[0m, in \u001b[0;36mexecute_query\u001b[0;34m(connection, query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute_query\u001b[39m(connection, query):\n\u001b[1;32m      2\u001b[0m     cursor \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39mcursor() \u001b[39m# se inicializa la conexión Creates a cursor object. Each statement will be executed in a new cursor object.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(query)\n\u001b[1;32m      4\u001b[0m     cursor\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/airflow_venv/lib/python3.10/site-packages/snowflake/connector/cursor.py:803\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[0;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, file_stream)\u001b[0m\n\u001b[1;32m    799\u001b[0m     is_integrity_error \u001b[39m=\u001b[39m (\n\u001b[1;32m    800\u001b[0m         code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m100072\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    801\u001b[0m     )  \u001b[39m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[1;32m    802\u001b[0m     error_class \u001b[39m=\u001b[39m IntegrityError \u001b[39mif\u001b[39;00m is_integrity_error \u001b[39melse\u001b[39;00m ProgrammingError\n\u001b[0;32m--> 803\u001b[0m     Error\u001b[39m.\u001b[39;49merrorhandler_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnection, \u001b[39mself\u001b[39;49m, error_class, errvalue)\n\u001b[1;32m    804\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/airflow_venv/lib/python3.10/site-packages/snowflake/connector/errors.py:275\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merrorhandler_wrapper\u001b[39m(\n\u001b[1;32m    254\u001b[0m     connection: SnowflakeConnection \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     error_value: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mbool\u001b[39m \u001b[39m|\u001b[39m \u001b[39mint\u001b[39m],\n\u001b[1;32m    258\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[39m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \n\u001b[1;32m    261\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39m        exception to the first handler in that order.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m     handed_over \u001b[39m=\u001b[39m Error\u001b[39m.\u001b[39;49mhand_to_other_handler(\n\u001b[1;32m    276\u001b[0m         connection,\n\u001b[1;32m    277\u001b[0m         cursor,\n\u001b[1;32m    278\u001b[0m         error_class,\n\u001b[1;32m    279\u001b[0m         error_value,\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m handed_over:\n\u001b[1;32m    282\u001b[0m         \u001b[39mraise\u001b[39;00m Error\u001b[39m.\u001b[39merrorhandler_make_exception(\n\u001b[1;32m    283\u001b[0m             error_class,\n\u001b[1;32m    284\u001b[0m             error_value,\n\u001b[1;32m    285\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/airflow_venv/lib/python3.10/site-packages/snowflake/connector/errors.py:330\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m cursor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     cursor\u001b[39m.\u001b[39mmessages\u001b[39m.\u001b[39mappend((error_class, error_value))\n\u001b[0;32m--> 330\u001b[0m     cursor\u001b[39m.\u001b[39;49merrorhandler(connection, cursor, error_class, error_value)\n\u001b[1;32m    331\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39melif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/airflow_venv/lib/python3.10/site-packages/snowflake/connector/errors.py:209\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_errorhandler\u001b[39m(\n\u001b[1;32m    193\u001b[0m     connection: SnowflakeConnection,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     error_value: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m],\n\u001b[1;32m    197\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39m\"\"\"Default error handler that raises an error.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \n\u001b[1;32m    200\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39m        A Snowflake error.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(\n\u001b[1;32m    210\u001b[0m         msg\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmsg\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    211\u001b[0m         errno\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39merrno\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    212\u001b[0m         sqlstate\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msqlstate\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    213\u001b[0m         sfqid\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msfqid\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    214\u001b[0m         done_format_msg\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdone_format_msg\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    215\u001b[0m         connection\u001b[39m=\u001b[39mconnection,\n\u001b[1;32m    216\u001b[0m         cursor\u001b[39m=\u001b[39mcursor,\n\u001b[1;32m    217\u001b[0m     )\n",
      "\u001b[0;31mProgrammingError\u001b[0m: 001003 (42000): SQL compilation error:\nsyntax error line 1 at position 4 unexpected 'ID_CONTINENTE'."
     ]
    }
   ],
   "source": [
    "sql = f\"PUT {file} @DATA_STAGE auto_compress=true\"\n",
    "    \n",
    "execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\n",
    "from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "args = {\"owner\": \"Airflow\", \"start_date\": datetime(2021,3,22,17,15)}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"snowflake_connector3\", default_args=args, schedule_interval=None\n",
    ")\n",
    "\n",
    "query1 = [\n",
    "    \"\"\"select 1;\"\"\",\n",
    "    \"\"\"show tables in database abcd_db;\"\"\",\n",
    "]\n",
    "\n",
    "def count1(**context):\n",
    "    dwh_hook = SnowflakeHook(snowflake_conn_id=\"snowflake_conn\")\n",
    "    result = dwh_hook.get_first(\"select count(*) from abcd_db.public.test3\")\n",
    "    logging.info(\"Number of rows in `abcd_db.public.test3`  - %s\", result[0])\n",
    "\n",
    "\n",
    "with dag:\n",
    "    query1_exec = SnowflakeOperator(\n",
    "        task_id=\"snowfalke_task1\",\n",
    "        sql=query1,\n",
    "        snowflake_conn_id=\"snowflake_conn\",\n",
    "    )\n",
    "\n",
    "    count_query = PythonOperator(task_id=\"count_query\", python_callable=count1)\n",
    "query1_exec >> count_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "\"\"\"\n",
    "Example use of Snowflake related operators.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "SNOWFLAKE_CONN_ID = 'my_snowflake_conn'\n",
    "\n",
    "\n",
    "SLACK_CONN_ID = 'my_slack_conn'\n",
    "\n",
    "# TODO: should be able to rely on connection's schema, but currently param required by S3ToSnowflakeTransfer\n",
    "\n",
    "SNOWFLAKE_SCHEMA = 'schema_name'\n",
    "SNOWFLAKE_STAGE = 'stage_name'\n",
    "SNOWFLAKE_WAREHOUSE = 'warehouse_name'\n",
    "SNOWFLAKE_DATABASE = 'database_name'\n",
    "SNOWFLAKE_ROLE = 'role_name'\n",
    "SNOWFLAKE_SAMPLE_TABLE = 'sample_table'\n",
    "\n",
    "\n",
    "GITHUB_FILE_PATH = '</datasets/sample_file.csv'\n",
    "\n",
    "# SQL commands\n",
    "\n",
    "CREATE_TABLE_SQL_STRING = (\n",
    "    f\"CREATE OR REPLACE TRANSIENT TABLE {SNOWFLAKE_SAMPLE_TABLE} (name VARCHAR(250), id INT);\"\n",
    "\n",
    ")\n",
    "\n",
    "SQL_INSERT_STATEMENT = f\"INSERT INTO {SNOWFLAKE_SAMPLE_TABLE} VALUES ('name', %(id)s)\"\n",
    "\n",
    "SQL_LIST = [SQL_INSERT_STATEMENT % {\"id\": n} for n in range(0, 10)]\n",
    "\n",
    "SQL_MULTIPLE_STMTS = \"; \".join(SQL_LIST)\n",
    "\n",
    "\n",
    "SNOWFLAKE_SLACK_SQL = f\"SELECT name, id FROM {SNOWFLAKE_SAMPLE_TABLE} LIMIT 10;\"\n",
    "\n",
    "\n",
    "SNOWFLAKE_SLACK_MESSAGE = (\n",
    "    \"Results in an ASCII table:\\n```{{ results_df | tabulate(tablefmt='pretty', headers='keys') }}```\"\n",
    "\n",
    ")\n",
    "\n",
    "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n",
    "\n",
    "\n",
    "DAG_ID = \"example_snowflake\"\n",
    "\n",
    "\n",
    "# [START howto_operator_snowflake]\n",
    "\n",
    "with DAG(\n",
    "    DAG_ID,\n",
    "    start_date=datetime(2021, 1, 1),\n",
    "    default_args={'snowflake_conn_id': SNOWFLAKE_CONN_ID},\n",
    "    tags=['example'],\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    # [START snowflake_example_dag]\n",
    "\n",
    "    snowflake_op_sql_str = SnowflakeOperator(\n",
    "        task_id='snowflake_op_sql_str',\n",
    "        sql=CREATE_TABLE_SQL_STRING,\n",
    "        warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "        database=SNOWFLAKE_DATABASE,\n",
    "        schema=SNOWFLAKE_SCHEMA,\n",
    "        role=SNOWFLAKE_ROLE,\n",
    "\n",
    "    )\n",
    "\n",
    "    snowflake_op_with_params = SnowflakeOperator(\n",
    "        task_id='snowflake_op_with_params',\n",
    "        sql=SQL_INSERT_STATEMENT,\n",
    "        parameters={\"id\": 56},\n",
    "        warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "        database=SNOWFLAKE_DATABASE,\n",
    "        schema=SNOWFLAKE_SCHEMA,\n",
    "        role=SNOWFLAKE_ROLE,\n",
    "    )\n",
    "\n",
    "    snowflake_op_sql_list = SnowflakeOperator(task_id='snowflake_op_sql_list', sql=SQL_LIST)\n",
    "\n",
    "    snowflake_op_sql_multiple_stmts = SnowflakeOperator(\n",
    "        task_id='snowflake_op_sql_multiple_stmts',\n",
    "        sql=SQL_MULTIPLE_STMTS,\n",
    "    )\n",
    "\n",
    "    snowflake_op_template_file = SnowflakeOperator(\n",
    "        task_id='snowflake_op_template_file',\n",
    "        sql='/path/to/sql/<filename>.sql',\n",
    "    )\n",
    "\n",
    "    # [END howto_operator_snowflake]\n",
    "\n",
    "    (\n",
    "        snowflake_op_sql_str\n",
    "        >> [\n",
    "            snowflake_op_with_params,\n",
    "            snowflake_op_sql_list,\n",
    "            snowflake_op_template_file,\n",
    "        \n",
    "            snowflake_op_sql_multiple_stmts,\n",
    "        ]\n",
    "    \n",
    "    )\n",
    "    # [END snowflake_example_dag]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.system.utils import get_test_run  # noqa: E402\n",
    "\n",
    "# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\n",
    "\n",
    "test_run = get_test_run(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\n",
    "from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "args = {\"owner\": \"Airflow\", \"start_date\": airflow.utils.dates.days_ago(2)}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"snowflake_automation\", default_args=args, schedule_interval=None\n",
    ")\n",
    "\n",
    "snowflake_query = [\n",
    "    \"\"\"create table public.test_employee (id number, name string);\"\"\",\n",
    "    \"\"\"insert into public.test_employee values(1, “Sam”),(2, “Andy”),(3, “Gill”);\"\"\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_row_count(**context):\n",
    "    dwh_hook = SnowflakeHook(snowflake_conn_id=\"snowflake_conn\")\n",
    "    result = dwh_hook.get_first(\"select count(*) from public.test_employee\")\n",
    "    logging.info(\"Number of rows in `public.test_employee`  - %s\", result[0])\n",
    "\n",
    "with dag:\n",
    "    create_insert = SnowflakeOperator(\n",
    "        task_id=\"snowfalke_create\",\n",
    "        sql=snowflake_query ,\n",
    "        snowflake_conn_id=\"snowflake_conn\",\n",
    "    )\n",
    "\n",
    "    get_count = PythonOperator(task_id=\"get_count\", python_callable=get_row_count)\n",
    "\n",
    "create_insert >> get_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('airflow_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "528625047c91465510bb71184b871ccd5df16f3e7542586ac1ad6d4e914cc60d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
