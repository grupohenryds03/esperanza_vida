{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se importa el conector de snowflake\n",
    "import snowflake.connector \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se crea conexión\n",
    "conn = snowflake.connector.connect(\n",
    "    user='grupods03',\n",
    "    password='Henry2022#',\n",
    "    account='nr28668.sa-east-1.aws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(connection, query):\n",
    "    cursor = connection.cursor() # se inicializa la conexión Creates a cursor object. Each statement will be executed in a new cursor object.\n",
    "    cursor.execute(query)\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"use database PRUEBA\" # se inicializa en database prueba\n",
    "\n",
    "execute_query(conn, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'create stage data_stage file_format = (type = \"csv\" field_delimiter = \",\" skip_header = 1)'\n",
    "execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '/Users/josetoledo/Desktop/prueba/df_hecho.csv'\n",
    "sql = \"PUT file://\" + csv_file + \" @DATA_STAGE auto_compress=true\"\n",
    "    \n",
    "execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tempfile\n",
    "import \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([(1,'America'), (2,'Europa'), (3,'Asia'),(4,'Africa'),(5,'Oceania')], columns=['ID_CONTINENTE', 'CONTINENTE']) # se crea dataframe con pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_CONTINENTE</th>\n",
       "      <th>CONTINENTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Europa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Oceania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_CONTINENTE CONTINENTE\n",
       "0              1    America\n",
       "1              2     Europa\n",
       "2              3       Asia\n",
       "3              4     Africa\n",
       "4              5    Oceania"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=df.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    df.to_csv(temp_dir +'/continente3.csv', index=False)\n",
    "    sql = f\"PUT file://{temp_dir+'/continente3.csv'} @DATA_STAGE auto_compress=true\"\n",
    "    execute_query(conn, sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data_stage/continente.csv.gz', 96, '1e953ff09ae314adf434d4c6267fcd40', 'Tue, 18 Oct 2022 13:45:58 GMT')\n",
      "('data_stage/continente2.csv.gz', 96, 'e02beef7c4a7d50b586656e6cae16cbb', 'Tue, 18 Oct 2022 13:46:30 GMT')\n",
      "('data_stage/df_hecho.csv.gz', 127424, 'cd7e24237c3bd0a69976527fd4b7219c', 'Thu, 13 Oct 2022 18:20:38 GMT')\n",
      "('data_stage/tmpo1gv29nlcontinente.csv.gz', 112, 'dd43809466184fbbedba4c43905058d8', 'Tue, 18 Oct 2022 13:44:58 GMT')\n",
      "('data_stage/tmpxq_5c9d6continente.csv.gz', 112, '7329701b007f46b2e4ffe985d9fdd45e', 'Tue, 18 Oct 2022 13:45:09 GMT')\n",
      "('data_stage/tmpz3m6_iqx.csv.gz', 96, '88121536ce05b77ed36c5a69d20434e7', 'Tue, 18 Oct 2022 13:08:44 GMT')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql='list @DATA_STAGE'\n",
    "cur=conn.cursor()\n",
    "cur.execute(sql)\n",
    "for c in cur:\n",
    "    print(c)\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\n",
    "from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "args = {\"owner\": \"Airflow\", \"start_date\": datetime(2021,3,22,17,15)}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"snowflake_connector3\", default_args=args, schedule_interval=None\n",
    ")\n",
    "\n",
    "query1 = [\n",
    "    \"\"\"select 1;\"\"\",\n",
    "    \"\"\"show tables in database abcd_db;\"\"\",\n",
    "]\n",
    "\n",
    "def count1(**context):\n",
    "    dwh_hook = SnowflakeHook(snowflake_conn_id=\"snowflake_conn\")\n",
    "    result = dwh_hook.get_first(\"select count(*) from abcd_db.public.test3\")\n",
    "    logging.info(\"Number of rows in `abcd_db.public.test3`  - %s\", result[0])\n",
    "\n",
    "\n",
    "with dag:\n",
    "    query1_exec = SnowflakeOperator(\n",
    "        task_id=\"snowfalke_task1\",\n",
    "        sql=query1,\n",
    "        snowflake_conn_id=\"snowflake_conn\",\n",
    "    )\n",
    "\n",
    "    count_query = PythonOperator(task_id=\"count_query\", python_callable=count1)\n",
    "query1_exec >> count_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "\"\"\"\n",
    "Example use of Snowflake related operators.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se crea conexión\n",
    "conn = snowflake.connector.connect(\n",
    "    user='grupods03',\n",
    "    password='Henry2022#',\n",
    "    account='nr28668.sa-east-1.aws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SNOWFLAKE_CONN_ID = 'my_snowflake_conn'\n",
    "SNOWFLAKE_STAGE = 'stage_name'\n",
    "SNOWFLAKE_WAREHOUSE = 'warehouse_name'\n",
    "SNOWFLAKE_DATABASE = 'database_name'\n",
    "SNOWFLAKE_ROLE = 'role_name'\n",
    "SNOWFLAKE_SAMPLE_TABLE = 'sample_table'\n",
    "\n",
    "\n",
    "GITHUB_FILE_PATH = '</datasets/sample_file.csv'\n",
    "\n",
    "# SQL commands\n",
    "\n",
    "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n",
    "\n",
    "DAG_ID = \"example_snowflake\"\n",
    "\n",
    "# [START howto_operator_snowflake]\n",
    "\n",
    "with DAG(\n",
    "    DAG_ID,\n",
    "    start_date=datetime(2021, 1, 1),\n",
    "    default_args={'snowflake_conn_id': SNOWFLAKE_CONN_ID},\n",
    "    tags=['example'],\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    # [START snowflake_example_dag]\n",
    "\n",
    "    snowflake_op_with_params = SnowflakeOperator(\n",
    "        task_id='snowflake_op_with_params',\n",
    "        sql=SQL_INSERT_STATEMENT,\n",
    "        parameters={\"id\": 56},\n",
    "        warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "        database=SNOWFLAKE_DATABASE,\n",
    "        schema=SNOWFLAKE_SCHEMA,\n",
    "        role=SNOWFLAKE_ROLE,\n",
    "    )\n",
    "\n",
    "    snowflake_op_sql_list = SnowflakeOperator(task_id='snowflake_op_sql_list', sql=SQL_LIST)\n",
    "\n",
    "    snowflake_op_sql_multiple_stmts = SnowflakeOperator(\n",
    "        task_id='snowflake_op_sql_multiple_stmts',\n",
    "        sql=SQL_MULTIPLE_STMTS,\n",
    "    )\n",
    "\n",
    "    snowflake_op_template_file = SnowflakeOperator(\n",
    "        task_id='snowflake_op_template_file',\n",
    "        sql='/path/to/sql/<filename>.sql',\n",
    "    )\n",
    "\n",
    "    # [END howto_operator_snowflake]\n",
    "\n",
    "    (\n",
    "        snowflake_op_sql_str\n",
    "        >> [\n",
    "            snowflake_op_with_params,\n",
    "            snowflake_op_sql_list,\n",
    "            snowflake_op_template_file,\n",
    "        \n",
    "            snowflake_op_sql_multiple_stmts,\n",
    "        ]\n",
    "    \n",
    "    )\n",
    "    # [END snowflake_example_dag]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "SNOWFLAKE_CONN_ID = 'my_snowflake_conn'\n",
    "\n",
    "# TODO: should be able to rely on connection's schema, but currently param required by S3ToSnowflakeTransfer\n",
    "\n",
    "SNOWFLAKE_SCHEMA = 'schema_name'\n",
    "SNOWFLAKE_STAGE = 'stage_name'\n",
    "SNOWFLAKE_WAREHOUSE = 'warehouse_name'\n",
    "SNOWFLAKE_DATABASE = 'database_name'\n",
    "SNOWFLAKE_ROLE = 'role_name'\n",
    "SNOWFLAKE_SAMPLE_TABLE = 'sample_table'\n",
    "\n",
    "\n",
    "GITHUB_FILE_PATH = '</datasets/sample_file.csv'\n",
    "\n",
    "# SQL commands\n",
    "\n",
    "CREATE_TABLE_SQL_STRING = (\n",
    "    f\"CREATE OR REPLACE TRANSIENT TABLE {SNOWFLAKE_SAMPLE_TABLE} (name VARCHAR(250), id INT);\"\n",
    "\n",
    ")\n",
    "\n",
    "SQL_INSERT_STATEMENT = f\"INSERT INTO {SNOWFLAKE_SAMPLE_TABLE} VALUES ('name', %(id)s)\"\n",
    "\n",
    "SQL_LIST = [SQL_INSERT_STATEMENT % {\"id\": n} for n in range(0, 10)]\n",
    "\n",
    "SQL_MULTIPLE_STMTS = \"; \".join(SQL_LIST)\n",
    "\n",
    "\n",
    "SNOWFLAKE_SLACK_SQL = f\"SELECT name, id FROM {SNOWFLAKE_SAMPLE_TABLE} LIMIT 10;\"\n",
    "\n",
    "\n",
    "SNOWFLAKE_SLACK_MESSAGE = (\n",
    "    \"Results in an ASCII table:\\n```{{ results_df | tabulate(tablefmt='pretty', headers='keys') }}```\"\n",
    "\n",
    ")\n",
    "\n",
    "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n",
    "\n",
    "\n",
    "DAG_ID = \"example_snowflake\"\n",
    "\n",
    "\n",
    "# [START howto_operator_snowflake]\n",
    "\n",
    "with DAG(\n",
    "    DAG_ID,\n",
    "    start_date=datetime(2021, 1, 1),\n",
    "    default_args={'snowflake_conn_id': SNOWFLAKE_CONN_ID},\n",
    "    tags=['example'],\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    # [START snowflake_example_dag]\n",
    "\n",
    "    snowflake_op_sql_str = SnowflakeOperator(\n",
    "        task_id='snowflake_op_sql_str',\n",
    "        sql=CREATE_TABLE_SQL_STRING,\n",
    "        warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "        database=SNOWFLAKE_DATABASE,\n",
    "        schema=SNOWFLAKE_SCHEMA,\n",
    "        role=SNOWFLAKE_ROLE,\n",
    "\n",
    "    )\n",
    "\n",
    "    snowflake_op_with_params = SnowflakeOperator(\n",
    "        task_id='snowflake_op_with_params',\n",
    "        sql=SQL_INSERT_STATEMENT,\n",
    "        parameters={\"id\": 56},\n",
    "        warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "        database=SNOWFLAKE_DATABASE,\n",
    "        schema=SNOWFLAKE_SCHEMA,\n",
    "        role=SNOWFLAKE_ROLE,\n",
    "    )\n",
    "\n",
    "    snowflake_op_sql_list = SnowflakeOperator(task_id='snowflake_op_sql_list', sql=SQL_LIST)\n",
    "\n",
    "    snowflake_op_sql_multiple_stmts = SnowflakeOperator(\n",
    "        task_id='snowflake_op_sql_multiple_stmts',\n",
    "        sql=SQL_MULTIPLE_STMTS,\n",
    "    )\n",
    "\n",
    "    snowflake_op_template_file = SnowflakeOperator(\n",
    "        task_id='snowflake_op_template_file',\n",
    "        sql='/path/to/sql/<filename>.sql',\n",
    "    )\n",
    "\n",
    "    # [END howto_operator_snowflake]\n",
    "\n",
    "    (\n",
    "        snowflake_op_sql_str\n",
    "        >> [\n",
    "            snowflake_op_with_params,\n",
    "            snowflake_op_sql_list,\n",
    "            snowflake_op_template_file,\n",
    "        \n",
    "            snowflake_op_sql_multiple_stmts,\n",
    "        ]\n",
    "    \n",
    "    )\n",
    "    # [END snowflake_example_dag]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.system.utils import get_test_run  # noqa: E402\n",
    "\n",
    "# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\n",
    "\n",
    "test_run = get_test_run(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\n",
    "from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import snowflake.connector\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.hooks.base_hook import BaseHook\n",
    "from airflow.operators.python_operator import PythonOperator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials\n",
    "snowflake_username = BaseHook.get_connection('snowflake').login\n",
    "snowflake_password = BaseHook.get_connection('snowflake').password\n",
    "snowflake_account = BaseHook.get_connection('snowflake').host\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wb_hecho_df():\n",
    "    df = pd.DataFrame([(1,'America'), (2,'Europa'), (3,'Asia'),(4,'Africa'),(5,'Oceania')], columns=['ID_CONTINENTE', 'CONTINENTE']) # se crea dataframe con pandas\n",
    "    return df\n",
    "\n",
    "def upload_to_snowflake():\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        df.to_csv(temp_dir +'/wb_hecho.csv', index=False)\n",
    "        sql = f\"PUT file://{temp_dir+'/wb_hecho.csv'} @DATA_STAGE auto_compress=true\"\n",
    "        execute_query(conn, 'USE DATABASE PRUEBA')\n",
    "        execute_query(conn, sql)\n",
    "\n",
    "with DAG(\"my_dag\", # Dag id\n",
    "    start_date=datetime(2022, 10 ,19), # start date, the 1st of January 2021 \n",
    "    schedule='@yearly',  # Cron expression, here it is a preset of Airflow, @yearly means once every year.\n",
    "    catchup=False  # Catchup \n",
    "    ) as dag:\n",
    "\n",
    "        create_wb_hecho=PythonOperator(\n",
    "            task_id=\"creation_of_dataframe_wb\",\n",
    "            python_callable=create_wb_hecho_df)\n",
    "            \n",
    "        put_file_into_stage_snowflake=PythonOperator(\n",
    "            task_id=\"file_to_snowflake_stage\",\n",
    "            python_callable=upload_to_snowflake)\n",
    "\n",
    "        create_wb_hecho >> put_file_into_stage_snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user='grupods03',\n",
    "    password='Henry2022#',\n",
    "    account='nr28668.sa-east-1.aws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def execute_query(connection, query):\n",
    "    cursor = connection.cursor() # se inicializa la conexión Creates a cursor object. Each statement will be executed in a new cursor object.\n",
    "    cursor.execute(query)\n",
    "    cursor.close()\n",
    "\n",
    "    \n",
    "dir='https://raw.githubusercontent.com/grupohenryds03/esperanza_vida/main/datasets/hechos.csv?token=GHSAT0AAAAAAB2ENG2ZQEV5BRDAG5BZT6TIY2P7M7Q'\n",
    "sql = f\"PUT file://{dir} @DATA_STAGE auto_compress=true\"\n",
    "execute_query(conn, 'USE DATABASE PRUEBA')\n",
    "execute_query(conn, sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('airflow_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "528625047c91465510bb71184b871ccd5df16f3e7542586ac1ad6d4e914cc60d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
