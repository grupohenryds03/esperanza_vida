{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se importa el conector de snowflake\n",
    "import snowflake.connector \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se crea conexión\n",
    "conn = snowflake.connector.connect(\n",
    "    user='grupods03',\n",
    "    password='Henry2022#',\n",
    "    account='nr28668.sa-east-1.aws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(connection, query):\n",
    "    cursor = connection.cursor() # se inicializa la conexión Creates a cursor object. Each statement will be executed in a new cursor object.\n",
    "    cursor.execute(query)\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Create database DB_Pablo\" # se inicializa en database prueba\n",
    "\n",
    "execute_query(conn, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'drop stage if exists data_stage'\n",
    "execute_query(conn, sql)\n",
    "\n",
    "sql = 'create stage data_stage file_format = (type = \"csv\" field_delimiter = \",\" skip_header = 1)'\n",
    "execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '/Users/josetoledo/Desktop/prueba/df_hecho.csv'\n",
    "sql = \"PUT file://\" + csv_file + \" @DATA_STAGE auto_compress=true\"\n",
    "    \n",
    "execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\n",
    "from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "args = {\"owner\": \"Airflow\", \"start_date\": datetime(2021,3,22,17,15)}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"snowflake_connector3\", default_args=args, schedule_interval=None\n",
    ")\n",
    "\n",
    "query1 = [\n",
    "    \"\"\"select 1;\"\"\",\n",
    "    \"\"\"show tables in database abcd_db;\"\"\",\n",
    "]\n",
    "\n",
    "def count1(**context):\n",
    "    dwh_hook = SnowflakeHook(snowflake_conn_id=\"snowflake_conn\")\n",
    "    result = dwh_hook.get_first(\"select count(*) from abcd_db.public.test3\")\n",
    "    logging.info(\"Number of rows in `abcd_db.public.test3`  - %s\", result[0])\n",
    "\n",
    "\n",
    "with dag:\n",
    "    query1_exec = SnowflakeOperator(\n",
    "        task_id=\"snowfalke_task1\",\n",
    "        sql=query1,\n",
    "        snowflake_conn_id=\"snowflake_conn\",\n",
    "    )\n",
    "\n",
    "    count_query = PythonOperator(task_id=\"count_query\", python_callable=count1)\n",
    "query1_exec >> count_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "\"\"\"\n",
    "Example use of Snowflake related operators.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "SNOWFLAKE_CONN_ID = 'my_snowflake_conn'\n",
    "\n",
    "\n",
    "SLACK_CONN_ID = 'my_slack_conn'\n",
    "\n",
    "# TODO: should be able to rely on connection's schema, but currently param required by S3ToSnowflakeTransfer\n",
    "\n",
    "SNOWFLAKE_SCHEMA = 'schema_name'\n",
    "SNOWFLAKE_STAGE = 'stage_name'\n",
    "SNOWFLAKE_WAREHOUSE = 'warehouse_name'\n",
    "SNOWFLAKE_DATABASE = 'database_name'\n",
    "SNOWFLAKE_ROLE = 'role_name'\n",
    "SNOWFLAKE_SAMPLE_TABLE = 'sample_table'\n",
    "\n",
    "\n",
    "GITHUB_FILE_PATH = '</datasets/sample_file.csv'\n",
    "\n",
    "# SQL commands\n",
    "\n",
    "CREATE_TABLE_SQL_STRING = (\n",
    "    f\"CREATE OR REPLACE TRANSIENT TABLE {SNOWFLAKE_SAMPLE_TABLE} (name VARCHAR(250), id INT);\"\n",
    "\n",
    ")\n",
    "\n",
    "SQL_INSERT_STATEMENT = f\"INSERT INTO {SNOWFLAKE_SAMPLE_TABLE} VALUES ('name', %(id)s)\"\n",
    "\n",
    "SQL_LIST = [SQL_INSERT_STATEMENT % {\"id\": n} for n in range(0, 10)]\n",
    "\n",
    "SQL_MULTIPLE_STMTS = \"; \".join(SQL_LIST)\n",
    "\n",
    "\n",
    "SNOWFLAKE_SLACK_SQL = f\"SELECT name, id FROM {SNOWFLAKE_SAMPLE_TABLE} LIMIT 10;\"\n",
    "\n",
    "\n",
    "SNOWFLAKE_SLACK_MESSAGE = (\n",
    "    \"Results in an ASCII table:\\n```{{ results_df | tabulate(tablefmt='pretty', headers='keys') }}```\"\n",
    "\n",
    ")\n",
    "\n",
    "ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")\n",
    "\n",
    "\n",
    "DAG_ID = \"example_snowflake\"\n",
    "\n",
    "\n",
    "# [START howto_operator_snowflake]\n",
    "\n",
    "with DAG(\n",
    "    DAG_ID,\n",
    "    start_date=datetime(2021, 1, 1),\n",
    "    default_args={'snowflake_conn_id': SNOWFLAKE_CONN_ID},\n",
    "    tags=['example'],\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    # [START snowflake_example_dag]\n",
    "\n",
    "    snowflake_op_sql_str = SnowflakeOperator(\n",
    "        task_id='snowflake_op_sql_str',\n",
    "        sql=CREATE_TABLE_SQL_STRING,\n",
    "        warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "        database=SNOWFLAKE_DATABASE,\n",
    "        schema=SNOWFLAKE_SCHEMA,\n",
    "        role=SNOWFLAKE_ROLE,\n",
    "\n",
    "    )\n",
    "\n",
    "    snowflake_op_with_params = SnowflakeOperator(\n",
    "        task_id='snowflake_op_with_params',\n",
    "        sql=SQL_INSERT_STATEMENT,\n",
    "        parameters={\"id\": 56},\n",
    "        warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "        database=SNOWFLAKE_DATABASE,\n",
    "        schema=SNOWFLAKE_SCHEMA,\n",
    "        role=SNOWFLAKE_ROLE,\n",
    "    )\n",
    "\n",
    "    snowflake_op_sql_list = SnowflakeOperator(task_id='snowflake_op_sql_list', sql=SQL_LIST)\n",
    "\n",
    "    snowflake_op_sql_multiple_stmts = SnowflakeOperator(\n",
    "        task_id='snowflake_op_sql_multiple_stmts',\n",
    "        sql=SQL_MULTIPLE_STMTS,\n",
    "    )\n",
    "\n",
    "    snowflake_op_template_file = SnowflakeOperator(\n",
    "        task_id='snowflake_op_template_file',\n",
    "        sql='/path/to/sql/<filename>.sql',\n",
    "    )\n",
    "\n",
    "    # [END howto_operator_snowflake]\n",
    "\n",
    "    (\n",
    "        snowflake_op_sql_str\n",
    "        >> [\n",
    "            snowflake_op_with_params,\n",
    "            snowflake_op_sql_list,\n",
    "            snowflake_op_template_file,\n",
    "        \n",
    "            snowflake_op_sql_multiple_stmts,\n",
    "        ]\n",
    "    \n",
    "    )\n",
    "    # [END snowflake_example_dag]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.system.utils import get_test_run  # noqa: E402\n",
    "\n",
    "# Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)\n",
    "\n",
    "test_run = get_test_run(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\n",
    "from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "args = {\"owner\": \"Airflow\", \"start_date\": airflow.utils.dates.days_ago(2)}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"snowflake_automation\", default_args=args, schedule_interval=None\n",
    ")\n",
    "\n",
    "snowflake_query = [\n",
    "    \"\"\"create table public.test_employee (id number, name string);\"\"\",\n",
    "    \"\"\"insert into public.test_employee values(1, “Sam”),(2, “Andy”),(3, “Gill”);\"\"\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_row_count(**context):\n",
    "    dwh_hook = SnowflakeHook(snowflake_conn_id=\"snowflake_conn\")\n",
    "    result = dwh_hook.get_first(\"select count(*) from public.test_employee\")\n",
    "    logging.info(\"Number of rows in `public.test_employee`  - %s\", result[0])\n",
    "\n",
    "with dag:\n",
    "    create_insert = SnowflakeOperator(\n",
    "        task_id=\"snowfalke_create\",\n",
    "        sql=snowflake_query ,\n",
    "        snowflake_conn_id=\"snowflake_conn\",\n",
    "    )\n",
    "\n",
    "    get_count = PythonOperator(task_id=\"get_count\", python_callable=get_row_count)\n",
    "\n",
    "create_insert >> get_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('airflow_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "528625047c91465510bb71184b871ccd5df16f3e7542586ac1ad6d4e914cc60d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
