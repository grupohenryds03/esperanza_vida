{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se importa el conector de snowflake\n",
    "import snowflake.connector \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se crea conexión\n",
    "conn = snowflake.connector.connect(\n",
    "    user='grupods03',\n",
    "    password='Henry2022#',\n",
    "    account='nr28668.sa-east-1.aws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(connection, query):\n",
    "    cursor = connection.cursor() # se inicializa la conexión Creates a cursor object. Each statement will be executed in a new cursor object.\n",
    "    cursor.execute(query)\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Create database DB_Pablo\" # se inicializa en database prueba\n",
    "\n",
    "execute_query(conn, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'drop stage if exists data_stage'\n",
    "execute_query(conn, sql)\n",
    "\n",
    "sql = 'create stage data_stage file_format = (type = \"csv\" field_delimiter = \",\" skip_header = 1)'\n",
    "execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '/Users/josetoledo/Desktop/prueba/df_hecho.csv'\n",
    "sql = \"PUT file://\" + csv_file + \" @DATA_STAGE auto_compress=true\"\n",
    "    \n",
    "execute_query(conn, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.contrib.hooks.snowflake_hook import SnowflakeHook\n",
    "from airflow.contrib.operators.snowflake_operator import SnowflakeOperator\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "args = {\"owner\": \"Airflow\", \"start_date\": datetime(2021,3,22,17,15)}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"snowflake_connector3\", default_args=args, schedule_interval=None\n",
    ")\n",
    "\n",
    "query1 = [\n",
    "    \"\"\"select 1;\"\"\",\n",
    "    \"\"\"show tables in database abcd_db;\"\"\",\n",
    "]\n",
    "\n",
    "\n",
    "def count1(**context):\n",
    "    dwh_hook = SnowflakeHook(snowflake_conn_id=\"snowflake_conn\")\n",
    "    result = dwh_hook.get_first(\"select count(*) from abcd_db.public.test3\")\n",
    "    logging.info(\"Number of rows in `abcd_db.public.test3`  - %s\", result[0])\n",
    "\n",
    "\n",
    "with dag:\n",
    "    query1_exec = SnowflakeOperator(\n",
    "        task_id=\"snowfalke_task1\",\n",
    "        sql=query1,\n",
    "        snowflake_conn_id=\"snowflake_conn\",\n",
    "    )\n",
    "\n",
    "    count_query = PythonOperator(task_id=\"count_query\", python_callable=count1)\n",
    "query1_exec >> count_query\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
